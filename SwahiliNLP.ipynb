{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook runs on kaggle below I mention steps to follow to run this notebook\n",
    "#1. I load the dataset consisiting trainset and  testset into the kaggle input folder and make the folder public inorder to use the dataset \n",
    "#2. I turn on GPu accelerator and enable internet in the kaggle platform\n",
    "#3. After the enviroment setup I commit my kernel by clicking save and run(commit) in the save version button\n",
    "#4. After the basic setup I start by running the code which I will briefly explain in every line of code\n",
    "#5. After running this code the output showed in the public leaderbord was 0.283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# load basic libraries \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Viewing the path to the dataset\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# update pip if not yet updated\n",
    "\n",
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install ktrain which is the wrapper for tensorflow keras that makes deep learning and Ai more accessible \n",
    "\n",
    "!pip install ktrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ktrain install tensor by default but if you are using other platform make sure you upgrade/downgrade tf to version 2.1.0 which ktrain runs\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and use text as ktrain\n",
    "\n",
    "import ktrain\n",
    "from ktrain import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import important API for text \n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(tweet):\n",
    "    \n",
    "  tweet = tweet.lower()                #convert text to lower-case\n",
    "  tweet = re.sub('â€˜','',tweet)    # remove the text â€˜ which appears to occur flequently\n",
    "  tweet = re.sub('â€™','',tweet)    # remove the text â€™ which appears to occur flequently\n",
    "    \n",
    "\n",
    "  tweet = word_tokenize(tweet)      # remove repeated characters (helloooooooo into hello)\n",
    "  return ' '.join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "MODEL_NAME = 'bert-base-multilingual-uncased'   # I used pretrained model \n",
    "\n",
    "\n",
    "t = text.Transformer(MODEL_NAME, maxlen= 128,  class_names=['kitaifa','michezo','biashara','kimataifa','burudani'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/Train.csv')   # load training dataset\n",
    "\n",
    "df[\"category\"] = df[\"category\"].str.lower()  # convert all text to lowercase\n",
    "\n",
    "test_path = '/kaggle/input/Test.csv'    # path to the test set in kaggle \n",
    "\n",
    "validation_set = pd.read_csv(test_path)\n",
    "validation_set[\"content\"] = validation_set[\"content\"].str.lower()  # convert to lowercase\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appply both training and test set to processtext function\n",
    "\n",
    "df['content'] = df['content'].apply(processText)   \n",
    "validation_set['content'] = validation_set['content'].apply(processText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize storage variable for the results from the test set\n",
    "\n",
    "valid_pred_ro = np.zeros((len(validation_set),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras load early stopping and model checkpoint which is used to load any best previous trained model based on parameter of interest\n",
    "\n",
    "# from sklearn load class weight \n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classweight is used to assign few label to have high loss compared to other\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(df['category']), df['category'])\n",
    "\n",
    "class_weight_dict = dict(enumerate(class_weights)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', patience= 3 , verbose=1, restore_best_weights=True)  # parameter of interest validation accuracy and training should stop if validation accuracy is below best value for 3 consequetive episode\n",
    "\n",
    "\n",
    "\n",
    "seed = 42\n",
    "n_folds = 10   # cros validation folds by running 10 folds it will guarantee the best results from developed model\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits= n_folds, random_state=seed, shuffle=False)  # stratified for balanced sampling of training sample\n",
    "\n",
    "n = 0  # Sometimes some folds produced worse results and then the model is skipped. n will guarantee the average is divided with only episodes contributing to the results\n",
    "\n",
    "for train_index, test_index in skf.split(df['content'], df['category']):\n",
    "    \n",
    "    \n",
    "    x_train, x_test = list(df.loc[train_index,'content']), list(df.loc[test_index,'content'])\n",
    "    y_train, y_test = np.asarray(df.loc[train_index,'category']), np.asarray(df.loc[test_index,'category'])\n",
    "    \n",
    "    trn = t.preprocess_train(x_train, y_train)\n",
    "    val = t.preprocess_test(x_test, y_test)\n",
    " \n",
    "    model = t.get_classifier()\n",
    "    \n",
    "    learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size= 6)\n",
    "    \n",
    "\n",
    "    \n",
    "    history = learner.fit(1e-5, 10, cycle_len=1, cycle_mult=2, class_weight= class_weight_dict, callbacks=[es], checkpoint_folder='/tmp')\n",
    "    \n",
    "    \n",
    "\n",
    "    learner.validate(class_names=t.get_classes())\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if max(history.history['val_accuracy']) < 0.8: # I used any model for testing set if max(history) of validation accuracy is above or equal to 80% else continue and other CV\n",
    "        continue \n",
    "\n",
    "    # make inference if the above condition is met \n",
    "    \n",
    "    predictor = ktrain.get_predictor(learner.model,preproc=t )\n",
    "    data = validation_set['content']\n",
    "    data = np.asarray(data)\n",
    "    print(predictor.get_classes())\n",
    "    pred = predictor.predict(data,return_proba=True)\n",
    "    n = n+1\n",
    "\n",
    "    valid_pred_ro += pred\n",
    "\n",
    "valid_pred_ro /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(valid_pred_ro, columns= ['biashara', 'burudani', 'kimataifa', 'kitaifa', 'michezo'])  # generate dataframe to store results\n",
    "sub_df['swahili_id'] = validation_set['swahili_id']\n",
    "\n",
    "sub_df = sub_df[['swahili_id','kitaifa','michezo','biashara','kimataifa','burudani']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.head()   # print results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_FILE_NAME = 'submission_ro.csv'\n",
    "sub_df.to_csv(SUB_FILE_NAME, index=False)   # save output in the kaggle output file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
